{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is part of the [Machine Learning class](https://github.com/erachelson/MLclass) by [Emmanuel Rachelson](https://personnel.isae-supaero.fr/emmanuel-rachelson?lang=en).\n",
    "\n",
    "This notebook is joint work by Remy Priem (primary author), Morgane Menz, Mostafa Meliani, Joseph Morlier and Emmanuel Rachelson.\n",
    "\n",
    "License: CC-BY-SA-NC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size:22pt; line-height:25pt; font-weight:bold; text-align:center;\">Gaussian Process Regression (practice session)</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "A surrogate model is an engineering method used when an outcome of interest cannot be easily directly measured, so a model of the outcome is used instead. Most engineering design problems require experiments and/or simulations to evaluate design objective and constraint functions as function of the design variables. For example, in order to find the optimal airfoil shape for an aircraft wing, an engineer simulates the air flow around the wing for different shape variables (length, curvature, material, ...). For many real world problems, however, a single simulation can take many minutes, hours, or even days to complete. As a result, routine tasks such as design optimization, design space exploration, sensitivity analysis become near impossible since they require a great number of function evaluations.\n",
    "\n",
    "One way of alleviating this burden is by constructing approximation models, known as surrogate models, response surface models, *metamodels* or emulators, that mimic the behavior of the simulation model as closely as possible while being computationally cheaper to evaluate. Surrogate models are constructed using a data-driven approach. The exact, inner working of the simulation code is not assumed to be known (or even understood), solely the input-output behavior is important.\n",
    "\n",
    "There exist multiple ways to build an approximation of a function: Artificial Neural Networks, Radial Basis Functions, Support Vector Regressions... \n",
    "In all these metamodels (or surrogate models), a fundamental assumption is that the quantity of interest $y(x)$ can be written $(y(x) = \\hat{y}(x) + \\epsilon)$, where the residuals $\\epsilon$ are independently and identically distributed normal random variables, so that fitting the model $\\hat{y}(x)$ is performed by minimizing a measure over $\\epsilon$.\n",
    "\n",
    "For a practical example, take a look to this airfoil optimization tool: [http://mdolab.engin.umich.edu/webfoil](http://mdolab.engin.umich.edu/webfoil)\n",
    "\n",
    "It has been constructed by lot of offline computations and an excellent tool developed jointly by University of Michigan, Nasa, Onera and ISAE-SUPAERO called SMT: [https://github.com/SMTorg/SMT](https://github.com/SMTorg/SMT)\n",
    "\n",
    "The authors strongly encourage students to have a look to classical textbooks such as [1] (for the Machine learning community) or [2] (for the Aerospace engineering community).\n",
    "\n",
    "[1]  Carl Edward Rasmussen. Gaussian Processes in Machine Learning. In Advanced lectures on machine learning, pages 63–71. Springer, 2004. Available for download at [http://www.gaussianprocess.org/gpml](http://www.gaussianprocess.org/gpml).<br>\n",
    "[2]  Alexander Forrester, Andy Keane et al. Engineering design via surrogate modelling: a practical guide. John Wiley & Sons, 2008."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Gaussian Process Regression\n",
    "\n",
    "The main idea behind Kriging is that the 'errors' -or more accurately, the deviations from the base model- in the predicted values $\\hat{y}$, are not independent. Rather, we take the view that the errors are a systematic function of the locations of the samples.\n",
    "\n",
    "Please recall your [previous course on GPR](https://github.com/erachelson/MLclass/blob/master/5%20-%20Gaussian%20Processes/Gaussian%20Processes.ipynb).\n",
    "\n",
    "We wish to train a GPR model $\\mathcal{G} = \\lbrace \\mathbf{X}, \\mathbf{Y}, \\theta \\rbrace$ using the Squared Exponential function. The so-called SE Kernel is $k(x,x') =\\sigma_f^2\\exp\\left(-\\frac{(x-x')^2}{l^ 2}\\right)$.\n",
    "\n",
    "Given the input data $\\mathbf{y} = \\left[y_1,\\ldots,y_N\\right]$ and $\\mathbf{x} =  \\left[x_1,\\ldots,x_N\\right]$, and given a covariance kernel $k(x,x')$, a Gaussian Process regressor estimates the distribution of $y(x)$ as a Gaussian $\\mathcal{N}(\\mu,\\sigma)$ with: $\\mu = K_*(x)K^{-1} \\mathbf{y}$, $\\sigma(y)^2 = k(x,x) - K_*(x)K^{-1}K_*(x)^T$, where:\n",
    "\n",
    "$$K =\n",
    "\\begin{bmatrix}\n",
    "k(x_1,x_1) & \\ldots & k(x_1,x_N) \\\\\n",
    "\\vdots     & \\ddots & \\vdots \\\\\n",
    "k(x_1,x_N) & \\ldots & k(x_N,x_N)\n",
    "\\end{bmatrix}, \\\n",
    "K_*(x) = \\left[k(x_1, x), \\ldots, k(x_N,x)\\right]$$\n",
    "\n",
    "On the 1D example $f(x)= (x-3.5)\\sin{\\frac{(x-3.5)}{\\pi}}$ on $\\left [ 0, 25\\right ]$, we will have a naive approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def fun(point):\n",
    "    return np.atleast_2d((point-3.5)*np.sin((point-3.5)/(np.pi)))\n",
    "\n",
    "X_plot = np.atleast_2d(np.linspace(0, 25, 10000)).T\n",
    "Y_plot = fun(X_plot)\n",
    "#y_gpr , y_std = gpr.predict(X_plot , return_std=True)\n",
    "\n",
    "lines = []\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "true_fun, = ax.plot(X_plot,Y_plot)\n",
    "lines.append(true_fun)\n",
    "ax.set_title('$x \\sin{x}$ function')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cov_function(point_1,point_2,theta,sig):\n",
    "    theta = np.asarray(theta)\n",
    "    size = point_1.shape\n",
    "    dist = (point_1-point_2)**2\n",
    "    k_12 = (sig**2) * np.exp(- np.sum(dist / theta**2))\n",
    "    return k_12\n",
    "\n",
    "def cov_matrix(points,theta,sig):\n",
    "    theta = np.asarray(theta)\n",
    "    size = points.shape\n",
    "    K = np.zeros((size[0],size[0]))\n",
    "    for i,point in enumerate(points):\n",
    "        K[:,i] = np.array([cov_function(point,point_1,theta,sig) for point_1 in points])\n",
    "    return K\n",
    "\n",
    "def cov_vect(point,points,theta,sig):\n",
    "    theta = np.asarray(theta)\n",
    "    size = points.shape\n",
    "    K = np.array([cov_function(point,point_1,theta,sig) for point_1 in points])\n",
    "    return K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 1:\n",
    "\n",
    "<div class=\"alert alert-warning\">**Question 1.1**<br>\n",
    "Code the function that predicts the mean and the standard deviation of the Gaussian process.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/code1.py\n",
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code).\n",
    "\n",
    "def myGPpredict(x_new, x_data, y_data, K_inv, theta, sig):\n",
    "    # Your code here\n",
    "    return mu[0],sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">**Question 1.2**<br>\n",
    "Test several values of $\\theta = [l, \\sigma_f]$ with  $l$ defined as the length-scale  (of oscillations) and  $\\sigma_f$ the amplitude. Look at the RMSE and R2 score that give an information on the accuracy of the model. RMSE must be close to 0 and R2 must be close to 1. For a reminder about these metrics, you can refer to https://en.wikipedia.org/wiki/Root-mean-square_deviation and https://en.wikipedia.org/wiki/Coefficient_of_determination.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "x_test = np.atleast_2d(np.linspace(0,25,100)).T\n",
    "y_test = fun(x_test)\n",
    "\n",
    "x_data = np.atleast_2d([0,11,20,1,5,15,12,3,17]).T\n",
    "y_data = fun(x_data)\n",
    "\n",
    "\n",
    "X_plot = np.atleast_2d(np.linspace(0,25,1000)).T\n",
    "Y_plot = fun(X_plot)\n",
    "\n",
    "lines = []\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "true_fun, = ax.plot(X_plot,Y_plot)\n",
    "data, = ax.plot(x_data,y_data,linestyle='',marker='o')\n",
    "lines.append(true_fun)\n",
    "lines.append(data)\n",
    "ax.set_title('$x \\sin{x}$ function')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.legend(lines,['True function','Data'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = [3]\n",
    "sig = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/code2.py\n",
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RMSE = mean_squared_error(y_test,y_pred[:,0])\n",
    "R2 = r2_score(y_test,y_pred[:,0])\n",
    "print('RMSE = %.5f' %(RMSE)) \n",
    "print('R2 = %.5f' %(R2))\n",
    "\n",
    "Y_GP_plot = np.array([myGPpredict(x_t,x_data,y_data,K_inv,theta,sig) \\\n",
    "                   for x_t in X_plot])\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "true_fun, = ax.plot(X_plot,Y_plot)\n",
    "data, = ax.plot(x_data,y_data,linestyle='',marker='o')\n",
    "gp, = ax.plot(X_plot,Y_GP_plot[:,0],linestyle='--',color='g')\n",
    "un_gp = ax.fill_between(X_plot.T[0],Y_GP_plot[:,0]+3*Y_GP_plot[:,1],Y_GP_plot[:,0]-3*Y_GP_plot[:,1],alpha=0.3,color='g')\n",
    "lines = [true_fun,data,gp,un_gp]\n",
    "ax.set_title('$x \\sin{x}$ function')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.legend(lines,['True function','Data','GPR prediction','99 % confidence'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">**Question 1.3**<br>\n",
    "Conclude: Is there a way to find a $\\theta_{optimal} = \\theta^*$?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert-danger\"><a href=\"#answers1-3\" data-toggle=\"collapse\">**Correction (click to unhide):**</a><br>\n",
    "<div id=\"answers1-3\" class=\"collapse\">\n",
    "TODO\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Hyperparameters Optimization\n",
    "\n",
    "To train the model, the negative log marginal likelihood with respect to the hyper-parameters is minimized:\n",
    "\n",
    "$$-\\text{log}\\, p(\\mathbf{Y} \\mid \\mathbf{X}, \\theta) = \\frac{n}{2} \\log(\\frac{\\mathbf{Y}^\\top\\mathbf{K}^{-1}\\mathbf{Y}}{n}) + \\frac{1}{2}\\log\\mid\\mathbf{K}\\mid + \\,c,$$\n",
    "\n",
    "where $c$ is a constant and the matrix $\\mathbf{K}$ is a function of the hyper-parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 2:\n",
    "\n",
    "<div class=\"alert alert-warning\">**Question 2.1**<br>\n",
    "Plot a 1D graph of the Marginal Likelihood function for $l \\in [10^{-3},1.25]$ at $\\sigma_f$ fixed.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/code3.py\n",
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code).\n",
    "\n",
    "def likelihood(x_data,y_data,theta,sig):\n",
    "    # CODE HERE\n",
    "    return like[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.linspace(1e-3,25)\n",
    "\n",
    "like = np.array([likelihood(x_data,y_data,th,sig) for th in theta])\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "line = ax.plot(theta,like)\n",
    "ax.set_xlabel('Theta')\n",
    "ax.set_ylabel('Log Likelihood')\n",
    "ax.set_title('Log Likelihood regarding theta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">**Question 2.2**<br>\n",
    "Optimize the Marginal Likelihood function to find $l^*$.<br>Watch out: some optimizers are gradient-based and only converge to a local optimum (try different initializations).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize \n",
    "\n",
    "like_obj = lambda theta : likelihood(x_data,y_data,theta,sig)\n",
    "\n",
    "\n",
    "# Multistart to remove bad optimization results\n",
    "theta_start = np.linspace(1e-6,25,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/code4.py\n",
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code).\n",
    "\n",
    "help(minimize)\n",
    "opt_all = np.array([# CODE THE OPTIMIZATION HERE with th as the starting point]) for th in theta_start \\\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_success = opt_all[[opt_i['success'] for opt_i in opt_all]]\n",
    "obj_success = np.array([opt_i['fun'] for opt_i in opt_success])\n",
    "ind_min = np.argmin(obj_success)\n",
    "\n",
    "opt = opt_success[ind_min]\n",
    "theta_et = opt['x']\n",
    "\n",
    "print('Optimization results', opt)\n",
    "print('')\n",
    "print('Best theta is %.5f' %(theta_et))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Optimization\n",
    "\n",
    "Now we would like to minimize a black box function subject to boundary constraints. This function is very expensive to evaluate and only provides a scalar output (meaning that you don't have access to gradients or higher order derivatives). We will use Gaussian Process to solve this optimization problem. \n",
    "\n",
    "Right now you can use directly scikit-learn which includes all these operations in a single procedure. Here, the data are the points $x_{data} = \\left [0, 7, 25 \\right ]$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice 3\n",
    "\n",
    "The `fun` function from section I will play the role of our \"expensive to evaluate function\".\n",
    "\n",
    "<div class=\"alert alert-warning\">**Question 3.1**<br>\n",
    "Build the GP model with a square exponential kernel with scikit-learn toolbox knowing $(x_{data}, y_{data})$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor as GP\n",
    "from sklearn.gaussian_process.kernels import RBF as SEkernel\n",
    "\n",
    "x_data = np.atleast_2d([0,7,25]).T\n",
    "y_data = fun(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/code5.py\n",
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code).\n",
    "\n",
    "help(GP)\n",
    "\n",
    "# CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_GP_plot = gpr.predict(X_plot,return_std=True)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "true_fun, = ax.plot(X_plot,Y_plot)\n",
    "data, = ax.plot(x_data,y_data,linestyle='',marker='o')\n",
    "gp, = ax.plot(X_plot,Y_GP_plot[0],linestyle='--',color='g')\n",
    "sig_plus = Y_GP_plot[0]+3*np.atleast_2d(Y_GP_plot[1]).T\n",
    "sig_moins = Y_GP_plot[0]-3*np.atleast_2d(Y_GP_plot[1]).T\n",
    "un_gp = ax.fill_between(X_plot.T[0],sig_plus.T[0],sig_moins.T[0],alpha=0.3,color='g')\n",
    "lines = [true_fun,data,gp,un_gp]\n",
    "ax.set_title('$x \\sin{x}$ function')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.legend(lines,['True function','Data','GPR prediction','99 % confidence'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice 4\n",
    "\n",
    "Suppose you are confronted to an optimization problem where each evaluation of the objective function is very expensive: let's say 10 hours per evaluation. You want to solve this problem under *computational budget contraints*.\n",
    "\n",
    "Your challenge is: Minimize the $x \\mapsto x \\sin{(x)}$ function given a total computational budget of 10 points with the initial points $(x_{data},y_{data})$.\n",
    "\n",
    "<div class=\"alert alert-warning\">**Question 4.1**<br>\n",
    "Give at least 2 methods to optimize the true function thanks to Gaussian Processes. You don't have to code it explicitly, just give the main idea.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert-danger\"><a href=\"#answers4-1\" data-toggle=\"collapse\">**Correction (click to unhide):**</a><br>\n",
    "<div id=\"answers4-1\" class=\"collapse\">\n",
    "TODO\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice 5 : Bayesian Optimization\n",
    "\n",
    "Bayesian optimization is defined by Jonas Mockus in [3] as an optimization technique based upon the minimization of the expected deviation from the extremum of the studied function. \n",
    "\n",
    "[3]  J. Močkus.  On bayesian methods for seeking the extremum, pages 400–404. Springer Berlin Heidelberg, Berlin, Heidelberg, 1975.\n",
    "\n",
    "The objective function is treated as a black-box function. A Bayesian strategy sees the objective as a random function and places a prior over it. The prior captures our beliefs about the behavior of the function. After gathering the function evaluations, which are treated as data, the prior is updated to form the posterior distribution over the objective function. The posterior distribution, in turn, is used to construct an acquisition function (often also referred to as infill sampling criterion) that determines what the next query point should be.\n",
    "\n",
    "One of the earliest bodies of work on Bayesian optimisation is [4,5]. Kushner used [Wiener processes](https://en.wikipedia.org/wiki/Stochastic_process#Wiener_process) (Brownian motion processes) for one-dimensional problems. Kushner’s decision model was based on maximizing the probability of improvement, and included a parameter that controlled the trade-off between ‘more global’ and ‘more local’ optimization, in the same spirit as the Exploration/Exploitation trade-off.\n",
    "\n",
    "[4] Harold J Kushner. A Versatile Stochastic Model of a Function of Unknown and Time-Varying Form. vol. 5, pages 150–167, 08 1962.<br>\n",
    "[5] Harold J Kushner. A New Method of Locating the Maximum Point of an Arbitrary Multipeak Curve in the Presence of Noise. vol. 86, 01 1964.\n",
    "\n",
    "Meanwhile, in the former Soviet Union, Mockus and colleagues developed a multidimensional Bayesian optimization method using linear combinations of Wiener fields, some of which was published in English in [3]. This paper also describes an acquisition function that is based on myopic expected improvement of the posterior, which has been widely adopted in Bayesian optimization as the Expected Improvement function.\n",
    "\n",
    "In 1998, Jones used Gaussian processes together with the expected improvement function to successfully perform derivative-free optimization and experimental design through an algorithm called  Efficient  Global  Optimization, or EGO [6].\n",
    "\n",
    "[6] Donald R. Jones, Matthias Schonlau and William J. Welch. Efficient Global Optimization of Expensive Black-Box Functions. J. of Global Optimization, vol. 13, no. 4, pages 455–492, 1998.\n",
    "\n",
    "## Efficient Global Optimization\n",
    "\n",
    "In what follows, we describe the Efficient Global Optimization (EGO) algorithm, as published in |6].\n",
    "\n",
    "Let $F$ be an expensive black-box function to be minimized. We sample $F$ at the different locations  $X = \\{x_1, x_2,\\ldots,x_n\\}$ yielding the responses $Y = \\{y_1, y_2,\\ldots,y_n\\}$. We build a Kriging model (also called Gaussian process) with a mean function $\\mu$ and a variance function $\\sigma^{2}$.\n",
    "\n",
    "The next step is to compute the criterion EI. To do this, let us write:\n",
    "\n",
    "$$f_{min} = \\min \\{y_1, y_2,\\ldots,y_n\\}.$$\n",
    "\n",
    "The Expected Improvement funtion (EI) can be expressed as:\n",
    "\n",
    "$$E[I(x)] = E[\\max(f_{min}-Y, 0)],$$\n",
    "\n",
    "where $Y$ is the random variable following the distribution $\\mathcal{N}(\\mu(x), \\sigma^{2}(x))$.\n",
    "By rewriting the right-hand side of EI's expression as an integral, and applying some tedious integration by parts, one can express the expected improvement in closed form:\n",
    "\n",
    "\\begin{equation}\n",
    "E[I(x)] = (f_{min} - \\mu(x))\\Phi\\left(\\frac{f_{min} - \\mu(x)}{\\sigma(x)}\\right) + \\sigma(x) \\phi\\left(\\frac{f_{min} - \\mu(x)}{\\sigma(x)}\\right)\n",
    "\\label{eq:EI_simp}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\Phi(\\cdot)$ and $\\phi(\\cdot)$ are respectively the cumulative and probability density functions of $\\mathcal{N}(0,1)$.\n",
    "\n",
    "Next, we determine our next sampling point as :\n",
    "\\begin{align}\n",
    "x_{n+1} = \\arg \\max_{x} \\left(E[I(x)]\\right)\n",
    "\\end{align}\n",
    "\n",
    "We then test the response $y_{n+1}$ of our black-box function $F$ at $x_{n+1}$, rebuild the model taking into account the new information gained, and research the point of maximum expected improvement again.\n",
    "\n",
    "We summarize here the EGO algorithm:\n",
    "\n",
    "EGO(F, $n_{iter}$ \\# Find the best minimum of $\\operatorname{F}$ in $n_{iter}$ iterations  \n",
    "For ($i=0:n_{iter}$)  \n",
    "\n",
    "* $mod = {model}(X, Y)$  \\# surrogate model based on sample vectors $X$ and $Y$  \n",
    "* $f_{min} = \\min Y$  \n",
    "* $x_{i+1} = \\arg \\max {EI}(mod, f_{min})$ \\# choose $x$ that maximizes EI  \n",
    "* $y_{i+1} = {F}(x_{i+1})$ \\# Probe the function at most promising point $x_{i+1}$  \n",
    "* $X = [X,x_{i+1}]$  \n",
    "* $Y = [Y,y_{i+1}]$   \n",
    "* $i = i+1$  \n",
    "\n",
    "$f_{min} = \\min Y$  \n",
    "Return : $f_{min}$ \\# This is the best known solution after $n_{iter}$ iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">**Question 5.1**<br>\n",
    "Implement the Expected Improvement function.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/code6.py\n",
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code).\n",
    "\n",
    "def EI(GP,points,f_min):\n",
    "    # CODE HERE\n",
    "    return ei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_GP_plot = gpr.predict(X_plot,return_std=True)\n",
    "Y_EI_plot = EI(gpr,X_plot,np.min(y_data))\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "true_fun, = ax.plot(X_plot,Y_plot)\n",
    "data, = ax.plot(x_data,y_data,linestyle='',marker='o')\n",
    "gp, = ax.plot(X_plot,Y_GP_plot[0],linestyle='--',color='g')\n",
    "sig_plus = Y_GP_plot[0]+3*np.atleast_2d(Y_GP_plot[1]).T\n",
    "sig_moins = Y_GP_plot[0]-3*np.atleast_2d(Y_GP_plot[1]).T\n",
    "un_gp = ax.fill_between(X_plot.T[0],sig_plus.T[0],sig_moins.T[0],alpha=0.3,color='g')\n",
    "ax1 = ax.twinx()\n",
    "ei, = ax1.plot(X_plot,Y_EI_plot,color='red')\n",
    "lines = [true_fun,data,gp,un_gp,ei]\n",
    "ax.set_title('$x \\sin{x}$ function')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax1.set_ylabel('ei')\n",
    "fig.legend(lines,['True function','Data','GPR prediction','99 % confidence','Expected Improvement'],loc=[0.13,0.64])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">**Question 5.2**<br>\n",
    "Complete the code of the EGO method and compare it to other infill criteria.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SBO(GP,point):\n",
    "    res = GP.predict(point)\n",
    "    return res\n",
    "\n",
    "def UCB(GP,point):\n",
    "    pred = GP.predict(point,return_std=True)\n",
    "    return pred[0]-3.*pred[1]\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "x_data = np.atleast_2d([0,7,25]).T\n",
    "y_data = fun(x_data)\n",
    "\n",
    "n_iter = 17\n",
    "ker = SEkernel()\n",
    "gpr = gpr = GP(kernel=ker,n_restarts_optimizer=10,normalize_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/code7.py\n",
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code).\n",
    "\n",
    "for k in range(n_iter):\n",
    "    x_start = np.atleast_2d(np.random.rand(15)*25).T\n",
    "    f_min_k = np.min(y_data)\n",
    "    gpr.fit(x_data,y_data)\n",
    "    ## UNCOMMENT ONE OF THE INFILL CRITERIA\n",
    "    # obj_k = lambda x: -EI(gpr,np.atleast_2d(x),f_min_k)\n",
    "    # obj_k = lambda x: SBO(gpr,np.atleast_2d(x))\n",
    "    # obj_k = lambda x: UCB(gpr,np.atleast_2d(x))\n",
    "    \n",
    "    opt_all = np.array([# CODE THE OPTIMIZATION HERE WITH X_st AS STARTING POINT) for x_st in x_start])\n",
    "    opt_success = opt_all[[opt_i['success'] for opt_i in opt_all]]\n",
    "    obj_success = np.array([opt_i['fun'] for opt_i in opt_success])\n",
    "    ind_min = np.argmin(obj_success)\n",
    "    opt = opt_success[ind_min]\n",
    "    x_et_k = opt['x']\n",
    "    \n",
    "    y_et_k = fun(x_et_k)\n",
    "    \n",
    "    y_data = np.atleast_2d(np.append(y_data,y_et_k)).T\n",
    "    x_data = np.atleast_2d(np.append(x_data,x_et_k)).T\n",
    "    \n",
    "ind_best = np.argmin(y_data)\n",
    "x_opt = x_data[ind_best]\n",
    "y_opt = y_data[ind_best]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Results : X = %s, Y = %s' %(x_opt,y_opt))\n",
    "\n",
    "Y_GP_plot = gpr.predict(X_plot,return_std=True)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "true_fun, = ax.plot(X_plot,Y_plot)\n",
    "data, = ax.plot(x_data,y_data,linestyle='',marker='o')\n",
    "gp, = ax.plot(X_plot,Y_GP_plot[0],linestyle='--',color='g')\n",
    "sig_plus = Y_GP_plot[0]+3*np.atleast_2d(Y_GP_plot[1]).T\n",
    "sig_moins = Y_GP_plot[0]-3*np.atleast_2d(Y_GP_plot[1]).T\n",
    "un_gp = ax.fill_between(X_plot.T[0],sig_plus.T[0],sig_moins.T[0],alpha=0.3,color='g')\n",
    "lines = [true_fun,data,gp,un_gp]\n",
    "ax.set_title('$x \\sin{x}$ function')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.legend(lines,['True function','Data','GPR prediction','99 % confidence'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recap: Play with a web app from Rasmussen\n",
    "\n",
    "[https://drafts.distill.pub/gp](https://drafts.distill.pub/gp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
